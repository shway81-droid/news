"""ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë¡¤ëŸ¬"""
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Tuple


# ë„¤ì´ë²„ ì–¸ë¡ ì‚¬ ì½”ë“œ ë§¤í•‘
PRESS_CODES = {
    # ê²½ì œ ì‹ ë¬¸ (10ê°œ)
    '015': 'í•œêµ­ê²½ì œ',
    '009': 'ë§¤ì¼ê²½ì œ',
    '011': 'ì„œìš¸ê²½ì œ',
    '008': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
    '014': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
    '018': 'ì´ë°ì¼ë¦¬',
    '243': 'ì•„ì£¼ê²½ì œ',
    '648': 'ë¹„ì¦ˆë‹ˆìŠ¤ì›Œì¹˜',
    '366': 'ì¡°ì„ ë¹„ì¦ˆ',
    '024': 'ë§¤ê²½ì´ì½”ë…¸ë¯¸',
    # ì£¼ìš” ì¼ê°„ì§€ (10ê°œ)
    '023': 'ì¡°ì„ ì¼ë³´',
    '025': 'ì¤‘ì•™ì¼ë³´',
    '020': 'ë™ì•„ì¼ë³´',
    '028': 'í•œê²¨ë ˆ',
    '032': 'ê²½í–¥ì‹ ë¬¸',
    '005': 'êµ­ë¯¼ì¼ë³´',
    '022': 'ì„¸ê³„ì¼ë³´',
    '047': 'ì˜¤ë§ˆì´ë‰´ìŠ¤',
    '081': 'ì„œìš¸ì‹ ë¬¸',
    '469': 'í•œêµ­ì¼ë³´',
}

# ìˆ˜ì§‘ ëŒ€ìƒ ì–¸ë¡ ì‚¬ ì½”ë“œ
TARGET_ECONOMY = ['015', '009', '011', '008', '014', '018', '243', '648', '366', '024']  # ê²½ì œì§€ 10ê°œ
TARGET_DAILY = ['023', '025', '020', '028', '032', '005', '022', '047', '081', '469']    # ì¼ê°„ì§€ 10ê°œ


def fetch_press_headline(press_code: str) -> Dict:
    """íŠ¹ì • ì–¸ë¡ ì‚¬ì˜ í—¤ë“œë¼ì¸ ê¸°ì‚¬ 1ê°œ ìˆ˜ì§‘

    Args:
        press_code: ë„¤ì´ë²„ ì–¸ë¡ ì‚¬ ì½”ë“œ

    Returns:
        ë‰´ìŠ¤ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬
    """
    url = f'https://media.naver.com/press/{press_code}/newspaper'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
    except Exception as e:
        print(f"  âš ï¸ {PRESS_CODES.get(press_code, press_code)} ì ‘ì† ì‹¤íŒ¨: {e}")
        return {}

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì²« ë²ˆì§¸ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
    article_links = soup.select('a[href*="/article/"]')
    if not article_links:
        print(f"  âš ï¸ {PRESS_CODES.get(press_code, press_code)} ê¸°ì‚¬ ì—†ìŒ")
        return {}

    first_article = article_links[0]
    title = first_article.get_text(strip=True)
    link = first_article.get('href', '')

    return {
        'title': title,
        'source': PRESS_CODES.get(press_code, f'ì–¸ë¡ ì‚¬({press_code})'),
        'link': link,
        'press_code': press_code
    }


def fetch_naver_headlines() -> Tuple[List[Dict], List[Dict]]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ì–¸ë¡ ì‚¬ë³„ í—¤ë“œë¼ì¸ ìˆ˜ì§‘

    Returns:
        (ê²½ì œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸, ì¼ê°„ì§€ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸)
    """
    economy_news = fetch_economy_headlines()
    daily_news = fetch_daily_headlines()
    return economy_news, daily_news


def fetch_economy_headlines() -> List[Dict]:
    """ê²½ì œ ì‹ ë¬¸ í—¤ë“œë¼ì¸ë§Œ ìˆ˜ì§‘"""
    economy_news = []
    print("  ğŸ’° ê²½ì œ ì‹ ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    for press_code in TARGET_ECONOMY:
        news = fetch_press_headline(press_code)
        if news:
            economy_news.append(news)
            print(f"    âœ… {news['source']}: {news['title'][:40]}...")
    return economy_news


def fetch_daily_headlines() -> List[Dict]:
    """ì¼ê°„ì§€ í—¤ë“œë¼ì¸ë§Œ ìˆ˜ì§‘"""
    daily_news = []
    print("  ğŸ“° ì£¼ìš” ì¼ê°„ì§€ ìˆ˜ì§‘ ì¤‘...")
    for press_code in TARGET_DAILY:
        news = fetch_press_headline(press_code)
        if news:
            daily_news.append(news)
            print(f"    âœ… {news['source']}: {news['title'][:40]}...")
    return daily_news


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("=== ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===\n")

    economy, daily = fetch_naver_headlines()

    print(f"\nğŸ’° ê²½ì œ ì‹ ë¬¸: {len(economy)}/10ê°œ ìˆ˜ì§‘")
    for news in economy:
        print(f"  - {news['source']}: {news['link']}")

    print(f"\nğŸ“° ì£¼ìš” ì¼ê°„ì§€: {len(daily)}/10ê°œ ìˆ˜ì§‘")
    for news in daily:
        print(f"  - {news['source']}: {news['link']}")
