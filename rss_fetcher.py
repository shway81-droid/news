"""RSS í”¼ë“œ ìˆ˜ì§‘ ëª¨ë“ˆ"""
import feedparser
from datetime import datetime, timedelta, timezone
from dateutil import parser as date_parser
from typing import List, Dict
import requests

from config import RSS_FEEDS, HOURS_LIMIT


def fetch_all_feeds() -> List[Dict]:
    """ëª¨ë“  RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    all_news = []

    for feed_info in RSS_FEEDS:
        try:
            news_items = fetch_single_feed(feed_info["url"], feed_info["name"])
            all_news.extend(news_items)
            print(f"âœ… {feed_info['name']}: {len(news_items)}ê°œ ìˆ˜ì§‘")
        except Exception as e:
            print(f"âŒ {feed_info['name']} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    seen_titles = set()
    unique_news = []
    for news in all_news:
        if news["title"] not in seen_titles:
            seen_titles.add(news["title"])
            unique_news.append(news)

    # ìµœì‹ ìˆœ ì •ë ¬
    unique_news.sort(key=lambda x: x["published"], reverse=True)

    print(f"\nğŸ“° ì´ {len(unique_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    return unique_news


def fetch_single_feed(url: str, source_name: str) -> List[Dict]:
    """ë‹¨ì¼ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    news_items = []

    # User-Agent ì„¤ì •
    headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(url)

    if not feed.entries:
        return []

    # ì‹œê°„ ê¸°ì¤€ ì„¤ì • (ìµœê·¼ Nì‹œê°„)
    now = datetime.now(timezone.utc)
    time_limit = now - timedelta(hours=HOURS_LIMIT)

    for entry in feed.entries:
        try:
            # ë°œí–‰ì¼ íŒŒì‹±
            published = parse_date(entry)

            # ì‹œê°„ í•„í„°ë§ (ìµœê·¼ Nì‹œê°„ ë‚´)
            if published and published < time_limit:
                continue

            # ì œëª© ì •ë¦¬ (CDATA ì œê±°)
            title = clean_title(entry.get("title", ""))
            if not title:
                continue

            news_item = {
                "title": title,
                "link": entry.get("link", ""),
                "source": source_name,
                "published": published or now,
                "summary": clean_title(entry.get("summary", entry.get("description", ""))[:200])
            }
            news_items.append(news_item)

        except Exception as e:
            continue

    return news_items


def parse_date(entry) -> datetime:
    """RSS ì—”íŠ¸ë¦¬ì—ì„œ ë‚ ì§œ íŒŒì‹±"""
    date_fields = ["published", "updated", "pubDate", "date"]

    for field in date_fields:
        if hasattr(entry, field) and getattr(entry, field):
            try:
                parsed = date_parser.parse(getattr(entry, field))
                # timezoneì´ ì—†ìœ¼ë©´ UTCë¡œ ì„¤ì •
                if parsed.tzinfo is None:
                    parsed = parsed.replace(tzinfo=timezone.utc)
                return parsed
            except:
                continue

    # published_parsed ì‚¬ìš©
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        try:
            return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        except:
            pass

    return datetime.now(timezone.utc)


def clean_title(text: str) -> str:
    """ì œëª© ì •ë¦¬ (CDATA, HTML íƒœê·¸ ì œê±°)"""
    if not text:
        return ""

    # CDATA ì œê±°
    text = text.replace("<![CDATA[", "").replace("]]>", "")

    # HTML íƒœê·¸ ì œê±°
    import re
    text = re.sub(r"<[^>]+>", "", text)

    # ê³µë°± ì •ë¦¬
    text = " ".join(text.split())

    return text.strip()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    news = fetch_all_feeds()
    print("\n--- ìµœê·¼ ë‰´ìŠ¤ 5ê°œ ---")
    for i, item in enumerate(news[:5], 1):
        print(f"{i}. [{item['source']}] {item['title']}")
